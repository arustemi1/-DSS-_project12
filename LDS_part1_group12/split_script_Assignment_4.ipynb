{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install reverse_geocode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import csv\n",
    "# import hashlib\n",
    "# import reverse_geocode as rg\n",
    "# from pycountry_convert import country_alpha2_to_continent_code, convert_continent_code_to_continent_name\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "def load_holiday_data(holiday_file):\n",
    "    \"\"\"Load holiday data from a CSV file.\"\"\"\n",
    "    holidays = {}\n",
    "    with open(holiday_file, 'r') as hfile:\n",
    "        reader = csv.DictReader(hfile)\n",
    "        for row in reader:\n",
    "            holiday_date = row['Date']\n",
    "            try:\n",
    "                # Parse date in \"YYYY-MM-DD\" format (adjust if input format differs)\n",
    "                parsed_date = datetime.strptime(holiday_date, '%Y-%m-%d').date()\n",
    "                holidays[parsed_date.strftime('%Y-%m-%d')] = row['Holiday']\n",
    "            except ValueError as e:\n",
    "                print(f\"Error parsing holiday date '{holiday_date}': {e}\")\n",
    "    return holidays\n",
    "\n",
    "def create_date_dimension(crashes_file, output_file, holiday_file):\n",
    "    \"\"\"Enhanced Date Dimension with additional attributes.\"\"\"\n",
    "    holiday_data = load_holiday_data(holiday_file)\n",
    "    \n",
    "    unique_dates = {}\n",
    "    with open(crashes_file, 'r') as infile:\n",
    "        reader = csv.DictReader(infile)\n",
    "        for row in reader:\n",
    "            crash_date = row['CRASH_DATE']\n",
    "            try:\n",
    "                # Parse the date in the \"MM/DD/YYYY HH:MM:SS AM/PM\" format\n",
    "                parsed_date = datetime.strptime(crash_date, '%m/%d/%Y %I:%M:%S %p')\n",
    "                date_str = parsed_date.strftime('%Y-%m-%d')  # Convert to \"YYYY-MM-DD\" for uniqueness\n",
    "                if date_str not in unique_dates:\n",
    "                    unique_dates[date_str] = {\n",
    "                        'date_id': str(parsed_date.strftime('%Y%m%d')),  # YYYYMMDD format\n",
    "                        'month': int(parsed_date.month),\n",
    "                        'year': int(parsed_date.year),\n",
    "                        'quarter': int((parsed_date.month - 1) // 3 + 1),\n",
    "                        'day_of_week': str(parsed_date.strftime('%A')),\n",
    "                        'week_number': int(parsed_date.isocalendar()[1]),  # Week of the year\n",
    "                        'is_weekend': 1 if parsed_date.weekday() >= 5 else 0,  # Saturday and Sunday\n",
    "                        'is_holiday': 1 if date_str in holiday_data else 0  # Check holidays\n",
    "                       # 'holiday_name': holiday_data.get(date_str, '')  # Holiday name if present\n",
    "                    }\n",
    "            except ValueError as e:\n",
    "                print(f\"Error parsing date '{crash_date}': {e}\")\n",
    "\n",
    "    with open(output_file, 'w', newline='') as outfile:\n",
    "        fieldnames = [\n",
    "            'date_id', 'month', 'year', 'quarter',\n",
    "            'day_of_week', 'week_number', 'is_weekend', 'is_holiday' ]\n",
    "        writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(unique_dates.values())\n",
    "\n",
    "\n",
    "\n",
    "# def create_date_dimension(crashes_file, output_file, holiday_list=[]):\n",
    "#     \"\"\"Enhanced Date Dimension with additional attributes.\"\"\"    \n",
    "#     unique_dates = {}\n",
    "#     with open(crashes_file, 'r') as infile:\n",
    "#         reader = csv.DictReader(infile)\n",
    "#         for row in reader:\n",
    "#             crash_date = row['CRASH_DATE']\n",
    "#             try:\n",
    "#                 # Parse the date in the \"MM/DD/YYYY HH:MM:SS AM/PM\" format\n",
    "#                 parsed_date = datetime.strptime(crash_date, '%m/%d/%Y %I:%M:%S %p')\n",
    "#                 date_str = parsed_date.strftime('%Y-%m-%d')  # Convert to \"YYYY-MM-DD\" for uniqueness\n",
    "#                 if date_str not in unique_dates:\n",
    "#                     unique_dates[date_str] = {\n",
    "#                         'date_id': str(parsed_date.strftime('%Y%m%d')),  # YYYYMMDD format\n",
    "#                         #'day': parsed_date.day,\n",
    "#                         'month': int(parsed_date.month),\n",
    "#                         'year': int(parsed_date.year),\n",
    "#                         'quarter': int((parsed_date.month - 1) // 3 + 1),\n",
    "#                         'day_of_week': str(parsed_date.strftime('%A')),\n",
    "#                         'week_number': int(parsed_date.isocalendar()[1]),  # Week of the year\n",
    "#                         'is_weekend': 1 if parsed_date.weekday() >= 5 else 0,  # Saturday and Sunday\n",
    "#                         'is_holiday': 1 if date_str in holiday_list else 0,  # Check holidays\n",
    "#                     }\n",
    "#             except ValueError as e:\n",
    "#                 print(f\"Error parsing date '{crash_date}': {e}\")\n",
    "\n",
    "#     with open(output_file, 'w', newline='') as outfile:\n",
    "#         fieldnames = [\n",
    "#             'date_id', 'month', 'year', 'quarter',\n",
    "#             'day_of_week', 'week_number', 'is_weekend', 'is_holiday'\n",
    "#         ]\n",
    "#         writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
    "#         writer.writeheader()\n",
    "#         writer.writerows(unique_dates.values())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vehicle_dimension(vehicles_file, output_file):\n",
    "    \"\"\"\n",
    "    Create the Vehicle Dimension based on unique vehicle details.\n",
    "    Each row represents a unique vehicle.\n",
    "    \"\"\"\n",
    "    vehicle_dimension = {}\n",
    "\n",
    "    # Process vehicle data\n",
    "    with open(vehicles_file, 'r') as infile:\n",
    "        reader = csv.DictReader(infile)\n",
    "        for row in reader:\n",
    "            vehicle_id = row['VEHICLE_ID']\n",
    "            if vehicle_id not in vehicle_dimension:\n",
    "                vehicle_dimension[vehicle_id] = {\n",
    "                    'vehicle_id': vehicle_id,\n",
    "                    'crash_id': row['RD_NO'],  # Links to crash_id\n",
    "                    'vehicle_type': row.get('VEHICLE_TYPE', 'Unknown'),\n",
    "                    'manufacturer': row.get('MAKE', 'Unknown'),\n",
    "                    'model': row.get('MODEL', 'Unknown'),\n",
    "                    'registration_state': row.get('LIC_PLATE_STATE', 'Unknown'),\n",
    "                }\n",
    "\n",
    "    # Write Vehicle Dimension to output file\n",
    "    with open(output_file, 'w', newline='') as outfile:\n",
    "        fieldnames = ['vehicle_id', 'crash_id', 'vehicle_type', 'manufacturer', 'model', 'registration_state']\n",
    "        writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(vehicle_dimension.values())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def create_geography_dimension(crashes_file, output_file):\n",
    "    \"\"\"\n",
    "    Create the Geography Dimension with area_at_risk based on injury severity.\n",
    "    Assigns unique location IDs and calculates risk levels.\n",
    "    \"\"\"\n",
    "    injury_scores = {}\n",
    "\n",
    "    # Process crashes to calculate weighted injury scores\n",
    "    with open(crashes_file, 'r') as infile:\n",
    "        reader = csv.DictReader(infile)\n",
    "        for row in reader:\n",
    "            beat_id = row['BEAT_OF_OCCURRENCE']\n",
    "            try:\n",
    "                # Safely convert injury fields to integers\n",
    "                fatal_injuries = int(float(row['INJURIES_FATAL']))\n",
    "                incapacitating_injuries = int(float(row['INJURIES_INCAPACITATING']))\n",
    "                non_incapacitating_injuries = int(float(row['INJURIES_NON_INCAPACITATING']))\n",
    "            except (ValueError, KeyError):\n",
    "                # Default to 0 if any value is invalid or missing\n",
    "                fatal_injuries = incapacitating_injuries = non_incapacitating_injuries = 0\n",
    "\n",
    "            # Calculate weighted injury score\n",
    "            score = (\n",
    "                5 * fatal_injuries +\n",
    "                3 * incapacitating_injuries +\n",
    "                1 * non_incapacitating_injuries\n",
    "            )\n",
    "            if beat_id not in injury_scores:\n",
    "                injury_scores[beat_id] = 0\n",
    "            injury_scores[beat_id] += score\n",
    "\n",
    "    # Create geography dimension with risk levels and unique location IDs\n",
    "    unique_geographies = {}\n",
    "    location_id_map = {}  # Map unique combination to location_id\n",
    "    current_location_id = 1\n",
    "\n",
    "    with open(crashes_file, 'r') as infile:\n",
    "        reader = csv.DictReader(infile)\n",
    "        for row in reader:\n",
    "            beat_id = row['BEAT_OF_OCCURRENCE']\n",
    "            street_name = row['STREET_NAME']\n",
    "            street_number = row.get('STREET_NO', '')\n",
    "\n",
    "            # Create a unique key for combination\n",
    "            location_key = (beat_id, street_number, street_name)\n",
    "            if location_key not in location_id_map:\n",
    "                total_score = injury_scores.get(beat_id, 0)\n",
    "                # Assign risk levels based on thresholds\n",
    "                area_at_risk = (\n",
    "                    \"High\" if total_score > 150 else\n",
    "                    \"Medium\" if total_score > 75 else\n",
    "                    \"Low\"\n",
    "                )\n",
    "                location_id_map[location_key] = current_location_id\n",
    "                unique_geographies[current_location_id] = {\n",
    "                    'location_id': current_location_id,\n",
    "                    'beat_id': beat_id,\n",
    "                    'street_name': street_name,\n",
    "                    'street_number': street_number,\n",
    "                    'area_risk_level': area_at_risk,\n",
    "                }\n",
    "                current_location_id += 1\n",
    "\n",
    "    # Write Geography Dimension to output file\n",
    "    with open(output_file, 'w', newline='') as outfile:\n",
    "        fieldnames = ['location_id', 'beat_id', 'street_name', 'street_number', 'area_risk_level']\n",
    "        writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(unique_geographies.values())\n",
    "\n",
    "    return location_id_map  # Return the mapping for use in other dimensions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_person_dimension(people_file, output_file):\n",
    "    \"\"\"Extract unique people data and add a boolean is_under_21 attribute.\"\"\"\n",
    "    unique_people = {}\n",
    "    with open(people_file, 'r') as infile:\n",
    "        reader = csv.DictReader(infile)\n",
    "        for row in reader:\n",
    "            person_id = row['PERSON_ID']\n",
    "            if person_id not in unique_people:\n",
    "                try:\n",
    "                    # Check and handle invalid or missing AGE\n",
    "                    age = int(row['AGE']) if row['AGE'].isdigit() else -1  # Default to -1 if invalid\n",
    "                except ValueError:\n",
    "                    age = -1  # Default age if parsing fails\n",
    "\n",
    "                unique_people[person_id] = {\n",
    "                    'person_id': person_id,\n",
    "                    'age': age,\n",
    "                    'gender': row['SEX'],\n",
    "                    'role_in_crash': row['PERSON_TYPE'],\n",
    "                    'injury_severity': row['INJURY_CLASSIFICATION'],\n",
    "                    'is_under_21': age < 21 if age >= 0 else False,  # False if age is invalid\n",
    "                }\n",
    "\n",
    "    with open(output_file, 'w', newline='') as outfile:\n",
    "        fieldnames = ['person_id', 'age', 'gender', 'role_in_crash', 'injury_severity', 'is_under_21']\n",
    "        writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(unique_people.values())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import hashlib\n",
    "\n",
    "def create_cause_dimension(crashes_file, output_file):\n",
    "    with open(crashes_file, 'r') as infile:\n",
    "        all_crashcauses = {}\n",
    "        reader = csv.DictReader(infile)\n",
    "        for row in reader:\n",
    "            # Generate a unique key based on cause-related fields\n",
    "            unique_key = f\"{row['PRIM_CONTRIBUTORY_CAUSE']}|{row['SEC_CONTRIBUTORY_CAUSE']}|{row['ROADWAY_SURFACE_COND']}|{row['LIGHTING_CONDITION']}|{row['WEATHER_CONDITION']}|{row['POSTED_SPEED_LIMIT']}|{row['TRAFFIC_CONTROL_DEVICE']}|{row['DEVICE_CONDITION']}|{row['ALIGNMENT']}|{row['ROAD_DEFECT']}\"\n",
    "            \n",
    "            # Create a hashed cause_id for uniqueness\n",
    "            cause_id = hashlib.md5(unique_key.encode()).hexdigest()[:8]  # Short hash for readability\n",
    "            \n",
    "            if cause_id not in all_crashcauses:\n",
    "                all_crashcauses[cause_id] = {\n",
    "                    'cause_id': cause_id,\n",
    "                    'primary_cause': row['PRIM_CONTRIBUTORY_CAUSE'],\n",
    "                    'secondary_cause': row['SEC_CONTRIBUTORY_CAUSE'],\n",
    "                    'road_condition': row['ROADWAY_SURFACE_COND'],\n",
    "                    'lighting_condition': row['LIGHTING_CONDITION'],\n",
    "                    'weather_condition': row['WEATHER_CONDITION'],\n",
    "                    'speed_limit': row['POSTED_SPEED_LIMIT'],\n",
    "                    'traffic_control_device': row['TRAFFIC_CONTROL_DEVICE'],\n",
    "                    'device_condition': row['DEVICE_CONDITION'],\n",
    "                    'alignment': row['ALIGNMENT'],\n",
    "                    'road_defect': row['ROAD_DEFECT'],\n",
    "                }\n",
    "    \n",
    "    # Write the Cause Dimension to a CSV file\n",
    "    with open(output_file, 'w', newline='') as outfile:\n",
    "        fieldnames = ['cause_id', 'primary_cause', 'secondary_cause', 'road_condition',\n",
    "                      'lighting_condition', 'weather_condition', 'speed_limit',\n",
    "                      'traffic_control_device', 'device_condition', 'alignment', 'road_defect']\n",
    "        writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(all_crashcauses.values())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_crash_dimension(crashes_file, output_file):\n",
    "\n",
    "    crash_dimension = {}\n",
    "\n",
    "    # Process crashes data\n",
    "    with open(crashes_file, 'r') as infile:\n",
    "        reader = csv.DictReader(infile)\n",
    "        for row in reader:\n",
    "            crash_id = row['RD_NO']\n",
    "            crash_date = row['CRASH_DATE']\n",
    "            parsed_date = datetime.strptime(crash_date, '%m/%d/%Y %I:%M:%S %p')\n",
    "            date_str = parsed_date.strftime('%Y-%m-%d')  # Convert to \"YYYY-MM-DD\" for uniqueness\n",
    "            time_str = parsed_date.strftime('%H:%M:%S') # this format is good for SQL to automatically get hour without making new column\n",
    "            if crash_id not in crash_dimension:\n",
    "                crash_dimension[crash_id] = {\n",
    "                    'crash_id': crash_id,\n",
    "                    'crash_date': date_str,\n",
    "                    'crash_time': time_str,\n",
    "                    'num_units': row.get('NUM_UNITS', -1),\n",
    "                    'crash_severity_category': row.get('MOST_SEVERE_INJURY', 'Unknown')\n",
    "                }\n",
    "\n",
    "    # Write Vehicle Dimension to output file\n",
    "    with open(output_file, 'w', newline='') as outfile:\n",
    "        fieldnames = ['crash_id', 'crash_date', 'crash_time', 'num_units', 'crash_severity_category']\n",
    "        writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(crash_dimension.values())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_fact_table(crashes_file, people_file, cause_file, geography_dimension_map, output_file):\n",
    "    \"\"\"Create the Fact Table for crashes, linking dimensions using foreign keys.\"\"\"\n",
    "    fact_table = []\n",
    "\n",
    "    # Load Cause Dimension into a dictionary for quick lookup\n",
    "    cause_mapping = {}\n",
    "    with open(cause_file, 'r') as cause_infile:\n",
    "        reader = csv.DictReader(cause_infile)\n",
    "        for row in reader:\n",
    "            # Use the same key structure as in create_cause_dimension\n",
    "            unique_key = f\"{row['primary_cause']}|{row['secondary_cause']}|{row['road_condition']}|{row['lighting_condition']}|{row['weather_condition']}|{row['speed_limit']}|{row['traffic_control_device']}|{row['device_condition']}|{row['alignment']}|{row['road_defect']}\"\n",
    "            cause_mapping[unique_key] = row['cause_id']\n",
    "\n",
    "    # Process crash data and find cause_id\n",
    "    crashes = {}\n",
    "    with open(crashes_file, 'r') as infile:\n",
    "        reader = csv.DictReader(infile)\n",
    "        for row in reader:\n",
    "            crash_date = row['CRASH_DATE']\n",
    "            # Correct date parsing format\n",
    "            parsed_date = datetime.strptime(crash_date, '%m/%d/%Y %I:%M:%S %p')\n",
    "            date_id = parsed_date.strftime('%Y%m%d')\n",
    "\n",
    "            # Lookup cause_id from the Cause Dimension\n",
    "            unique_key = f\"{row['PRIM_CONTRIBUTORY_CAUSE']}|{row['SEC_CONTRIBUTORY_CAUSE']}|{row['ROADWAY_SURFACE_COND']}|{row['LIGHTING_CONDITION']}|{row['WEATHER_CONDITION']}|{row['POSTED_SPEED_LIMIT']}|{row['TRAFFIC_CONTROL_DEVICE']}|{row['DEVICE_CONDITION']}|{row['ALIGNMENT']}|{row['ROAD_DEFECT']}\"\n",
    "            cause_id = cause_mapping.get(unique_key, 'Unknown')  # Default to 'Unknown' if no match\n",
    "\n",
    "            # Get location_id from geography_dimension_map\n",
    "            location_key = (row['BEAT_OF_OCCURRENCE'], row.get('STREET_NO', ''), row['STREET_NAME'])\n",
    "            location_id = geography_dimension_map.get(location_key, 'Unknown')\n",
    "\n",
    "            crashes[row['RD_NO']] = {\n",
    "                'location_id': location_id,\n",
    "                'date_id': date_id,\n",
    "                'cause_id': cause_id\n",
    "            }\n",
    "\n",
    "    # Process people data and link to crashes\n",
    "    with open(people_file, 'r') as infile:\n",
    "        reader = csv.DictReader(infile)\n",
    "        for row in reader:\n",
    "            crash_id = row['RD_NO']\n",
    "            person_id = row['PERSON_ID']\n",
    "            vehicle_id = row.get('VEHICLE_ID', 'Unknown')  # Default if missing\n",
    "            damage_category = row.get('DAMAGE_CATEGORY', 'Unknown')  # Default if missing\n",
    "            damage = row['DAMAGE']  # Assuming this is the correct field name for damage\n",
    "\n",
    "            # Ensure the crash exists in crashes data\n",
    "            if crash_id in crashes:\n",
    "                fact = {\n",
    "                    'crash_id': crash_id,\n",
    "                    'cause_id': crashes[crash_id]['cause_id'],  # Use cause_id from lookup\n",
    "                    'person_id': person_id,\n",
    "                    'vehicle_id': vehicle_id,\n",
    "                    'location_id': crashes[crash_id]['location_id'],\n",
    "                    'date_id': crashes[crash_id]['date_id'],\n",
    "                    'damage': damage,\n",
    "                    'damage_category': f'{float(damage_category):.2f}' if damage_category.replace('.', '', 1).isdigit() else damage_category\n",
    "                }\n",
    "                fact_table.append(fact)\n",
    "\n",
    "    # Write the Fact Table to the output file\n",
    "    with open(output_file, 'w', newline='') as outfile:\n",
    "        fieldnames = ['crash_id', 'cause_id', 'person_id', 'vehicle_id', 'location_id', 'date_id', 'damage', 'damage_category']\n",
    "        writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(fact_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "create_date_dimension('Crashes_deduped.csv', 'DateDimension_withholiday.csv', 'holiday_in_usa.csv')\n",
    "create_geography_dimension('Crashes_deduped.csv', 'GeographyDimension_2.csv')\n",
    "create_person_dimension('People_damage_filled_CSV.csv', 'PersonDimension.csv')\n",
    "create_cause_dimension('Crashes_deduped.csv', 'CauseDimension.csv')\n",
    "create_vehicle_dimension('Vehicles_IDfilled_nosplit.csv', 'VehicleDimension.csv' )\n",
    "create_crash_dimension('Crashes_deduped.csv', 'CrashDimension.csv')\n",
    "\n",
    "\n",
    "location_id_map = create_geography_dimension('Crashes_deduped.csv', 'GeographyDimension_2.csv')\n",
    "\n",
    "\n",
    "create_fact_table('Crashes_deduped.csv', 'People_damage_filled_CSV.csv', 'CauseDimension.csv', location_id_map, 'FactTable_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
