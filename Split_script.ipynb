{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeableNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: reverse_geocode in c:\\users\\lenovo\\appdata\\roaming\\python\\python311\\site-packages (1.6.5)\n",
      "Requirement already satisfied: numpy in c:\\users\\lenovo\\appdata\\roaming\\python\\python311\\site-packages (from reverse_geocode) (1.26.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\lenovo\\appdata\\roaming\\python\\python311\\site-packages (from reverse_geocode) (1.13.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install reverse_geocode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import hashlib\n",
    "import reverse_geocode as rg\n",
    "from pycountry_convert import country_alpha2_to_continent_code, convert_continent_code_to_continent_name\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_date_dimension(crashes_file, output_file, holiday_list=[]):\n",
    "    \"\"\"Enhanced Date Dimension with additional attributes.\"\"\"    \n",
    "    unique_dates = {}\n",
    "    with open(crashes_file, 'r') as infile:\n",
    "        reader = csv.DictReader(infile)\n",
    "        for row in reader:\n",
    "            crash_date = row['CRASH_DATE']\n",
    "            try:\n",
    "                # Parse the date in the \"MM/DD/YYYY HH:MM:SS AM/PM\" format\n",
    "                parsed_date = datetime.strptime(crash_date, '%m/%d/%Y %I:%M:%S %p')\n",
    "                date_str = parsed_date.strftime('%Y-%m-%d')  # Convert to \"YYYY-MM-DD\" for uniqueness\n",
    "                if date_str not in unique_dates:\n",
    "                    unique_dates[date_str] = {\n",
    "                        'date_id': parsed_date.strftime('%Y%m%d'),  # YYYYMMDD format\n",
    "                        'day': parsed_date.day,\n",
    "                        'month': parsed_date.month,\n",
    "                        'year': parsed_date.year,\n",
    "                        'quarter': (parsed_date.month - 1) // 3 + 1,\n",
    "                        'day_of_week': parsed_date.strftime('%A'),\n",
    "                        'week_number': parsed_date.isocalendar()[1],  # Week of the year\n",
    "                        'is_weekend': 1 if parsed_date.weekday() >= 5 else 0,  # Saturday and Sunday\n",
    "                        'is_holiday': 1 if date_str in holiday_list else 0,  # Check holidays\n",
    "                    }\n",
    "            except ValueError as e:\n",
    "                print(f\"Error parsing date '{crash_date}': {e}\")\n",
    "\n",
    "    with open(output_file, 'w', newline='') as outfile:\n",
    "        fieldnames = [\n",
    "            'date_id', 'day', 'month', 'year', 'quarter',\n",
    "            'day_of_week', 'week_number', 'is_weekend', 'is_holiday'\n",
    "        ]\n",
    "        writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(unique_dates.values())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vehicle_dimension(vehicles_file, output_file):\n",
    "    \"\"\"\n",
    "    Create the Vehicle Dimension based on unique vehicle details.\n",
    "    Each row represents a unique vehicle.\n",
    "    \"\"\"\n",
    "    vehicle_dimension = {}\n",
    "\n",
    "    # Process vehicle data\n",
    "    with open(vehicles_file, 'r') as infile:\n",
    "        reader = csv.DictReader(infile)\n",
    "        for row in reader:\n",
    "            vehicle_id = row['VEHICLE_ID']\n",
    "            if vehicle_id not in vehicle_dimension:\n",
    "                vehicle_dimension[vehicle_id] = {\n",
    "                    'vehicle_id': vehicle_id,\n",
    "                    'crash_id': row['RD_NO'],  # Links to crash_id\n",
    "                    'vehicle_type': row.get('VEHICLE_TYPE', 'Unknown'),\n",
    "                    'manufacturer': row.get('MAKE', 'Unknown'),\n",
    "                    'model': row.get('MODEL', 'Unknown'),\n",
    "                    'registration_state': row.get('LIC_PLATE_STATE', 'Unknown'),\n",
    "                }\n",
    "\n",
    "    # Write Vehicle Dimension to output file\n",
    "    with open(output_file, 'w', newline='') as outfile:\n",
    "        fieldnames = ['vehicle_id', 'crash_id', 'vehicle_type', 'manufacturer', 'model', 'registration_state']\n",
    "        writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(vehicle_dimension.values())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_geography_dimension(crashes_file, output_file):\n",
    "    \"\"\"\n",
    "    Create the Geography Dimension with area_at_risk based on injury severity.\n",
    "    Assigns risk levels based on weighted injury scores.\n",
    "    \"\"\"\n",
    "    injury_scores = {}\n",
    "\n",
    "    # Process crashes to calculate weighted injury scores\n",
    "    with open(crashes_file, 'r') as infile:\n",
    "        reader = csv.DictReader(infile)\n",
    "        for row in reader:\n",
    "            beat_id = row['BEAT_OF_OCCURRENCE']\n",
    "            try:\n",
    "                # Safely convert injury fields to integers\n",
    "                fatal_injuries = int(float(row['INJURIES_FATAL']))\n",
    "                incapacitating_injuries = int(float(row['INJURIES_INCAPACITATING']))\n",
    "                non_incapacitating_injuries = int(float(row['INJURIES_NON_INCAPACITATING']))\n",
    "            except (ValueError, KeyError):\n",
    "                # Default to 0 if any value is invalid or missing\n",
    "                fatal_injuries = incapacitating_injuries = non_incapacitating_injuries = 0\n",
    "\n",
    "            # Calculate weighted injury score\n",
    "            score = (\n",
    "                5 * fatal_injuries +\n",
    "                3 * incapacitating_injuries +\n",
    "                1 * non_incapacitating_injuries\n",
    "            )\n",
    "            if beat_id not in injury_scores:\n",
    "                injury_scores[beat_id] = 0\n",
    "            injury_scores[beat_id] += score\n",
    "\n",
    "    # Create geography dimension with risk levels\n",
    "    unique_geographies = {}\n",
    "    with open(crashes_file, 'r') as infile:\n",
    "        reader = csv.DictReader(infile)\n",
    "        for row in reader:\n",
    "            beat_id = row['BEAT_OF_OCCURRENCE']\n",
    "            if beat_id not in unique_geographies:\n",
    "                total_score = injury_scores.get(beat_id, 0)\n",
    "                # Assign risk levels based on thresholds\n",
    "                area_at_risk = (\n",
    "                    \"High\" if total_score > 150 else\n",
    "                    \"Medium\" if total_score > 75 else\n",
    "                    \"Low\"\n",
    "                )\n",
    "                unique_geographies[beat_id] = {\n",
    "                    'beat_id': beat_id,\n",
    "                    'street_name': row['STREET_NAME'],\n",
    "                    'street_number': row.get('STREET_NO', ''),\n",
    "                    'area_risk_level': area_at_risk,\n",
    "                }\n",
    "\n",
    "    # Write Geography Dimension to output file\n",
    "    with open(output_file, 'w', newline='') as outfile:\n",
    "        fieldnames = ['beat_id', 'street_name', 'street_number', 'area_risk_level']\n",
    "        writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(unique_geographies.values())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_person_dimension(people_file, output_file):\n",
    "    \"\"\"Extract unique people data and add a boolean is_under_21 attribute.\"\"\"\n",
    "    unique_people = {}\n",
    "    with open(people_file, 'r') as infile:\n",
    "        reader = csv.DictReader(infile)\n",
    "        for row in reader:\n",
    "            person_id = row['PERSON_ID']\n",
    "            if person_id not in unique_people:\n",
    "                try:\n",
    "                    # Check and handle invalid or missing AGE\n",
    "                    age = int(row['AGE']) if row['AGE'].isdigit() else -1  # Default to -1 if invalid\n",
    "                except ValueError:\n",
    "                    age = -1  # Default age if parsing fails\n",
    "\n",
    "                unique_people[person_id] = {\n",
    "                    'person_id': person_id,\n",
    "                    'age': age,\n",
    "                    'gender': row['SEX'],\n",
    "                    'role_in_crash': row['PERSON_TYPE'],\n",
    "                    'injury_severity': row['INJURY_CLASSIFICATION'],\n",
    "                    'is_under_21': age < 21 if age >= 0 else False,  # False if age is invalid\n",
    "                }\n",
    "\n",
    "    with open(output_file, 'w', newline='') as outfile:\n",
    "        fieldnames = ['person_id', 'age', 'gender', 'role_in_crash', 'injury_severity', 'is_under_21']\n",
    "        writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(unique_people.values())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import hashlib\n",
    "\n",
    "def create_cause_dimension(crashes_file, output_file):\n",
    "    with open(crashes_file, 'r') as infile:\n",
    "        all_crashcauses = {}\n",
    "        reader = csv.DictReader(infile)\n",
    "        for row in reader:\n",
    "            # Generate a unique key based on cause-related fields\n",
    "            unique_key = f\"{row['PRIM_CONTRIBUTORY_CAUSE']}|{row['SEC_CONTRIBUTORY_CAUSE']}|{row['ROADWAY_SURFACE_COND']}|{row['LIGHTING_CONDITION']}|{row['WEATHER_CONDITION']}|{row['POSTED_SPEED_LIMIT']}|{row['TRAFFIC_CONTROL_DEVICE']}|{row['DEVICE_CONDITION']}|{row['ALIGNMENT']}|{row['ROAD_DEFECT']}\"\n",
    "            \n",
    "            # Create a hashed cause_id for uniqueness\n",
    "            cause_id = hashlib.md5(unique_key.encode()).hexdigest()[:8]  # Short hash for readability\n",
    "            \n",
    "            if cause_id not in all_crashcauses:\n",
    "                all_crashcauses[cause_id] = {\n",
    "                    'cause_id': cause_id,\n",
    "                    'primary_cause': row['PRIM_CONTRIBUTORY_CAUSE'],\n",
    "                    'secondary_cause': row['SEC_CONTRIBUTORY_CAUSE'],\n",
    "                    'road_condition': row['ROADWAY_SURFACE_COND'],\n",
    "                    'lighting_condition': row['LIGHTING_CONDITION'],\n",
    "                    'weather_condition': row['WEATHER_CONDITION'],\n",
    "                    'speed_limit': row['POSTED_SPEED_LIMIT'],\n",
    "                    'traffic_control_device': row['TRAFFIC_CONTROL_DEVICE'],\n",
    "                    'device_condition': row['DEVICE_CONDITION'],\n",
    "                    'alignment': row['ALIGNMENT'],\n",
    "                    'road_defect': row['ROAD_DEFECT'],\n",
    "                }\n",
    "    \n",
    "    # Write the Cause Dimension to a CSV file\n",
    "    with open(output_file, 'w', newline='') as outfile:\n",
    "        fieldnames = ['cause_id', 'primary_cause', 'secondary_cause', 'road_condition',\n",
    "                      'lighting_condition', 'weather_condition', 'speed_limit',\n",
    "                      'traffic_control_device', 'device_condition', 'alignment', 'road_defect']\n",
    "        writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(all_crashcauses.values())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_crash_dimension(crashes_file, output_file):\n",
    "\n",
    "    crash_dimension = {}\n",
    "\n",
    "    # Process crashes data\n",
    "    with open(crashes_file, 'r') as infile:\n",
    "        reader = csv.DictReader(infile)\n",
    "        for row in reader:\n",
    "            crash_id = row['RD_NO']\n",
    "            crash_date = row['CRASH_DATE']\n",
    "            parsed_date = datetime.strptime(crash_date, '%m/%d/%Y %I:%M:%S %p')\n",
    "            date_str = parsed_date.strftime('%Y-%m-%d')  # Convert to \"YYYY-MM-DD\" for uniqueness\n",
    "            time_str = parsed_date.strftime('%H:%M:%S') # this format is good for SQL to automatically get hour without making new column\n",
    "            if crash_id not in crash_dimension:\n",
    "                crash_dimension[crash_id] = {\n",
    "                    'crash_id': crash_id,\n",
    "                    'crash_date': date_str,\n",
    "                    'crash_time': time_str,\n",
    "                    'num_units': row.get('NUM_UNITS', -1),\n",
    "                    'crash_severity_category': row.get('MOST_SEVERE_INJURY', 'Unknown')\n",
    "                }\n",
    "\n",
    "    # Write Vehicle Dimension to output file\n",
    "    with open(output_file, 'w', newline='') as outfile:\n",
    "        fieldnames = ['crash_id', 'crash_date', 'crash_time', 'num_units', 'crash_severity_category']\n",
    "        writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(crash_dimension.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "def create_fact_table(crashes_file, people_file, cause_file, output_file):\n",
    "    \"\"\"Create the Fact Table for crashes, linking dimensions using foreign keys.\"\"\"\n",
    "    fact_table = []\n",
    "\n",
    "    # Load Cause Dimension into a dictionary for quick lookup\n",
    "    cause_mapping = {}\n",
    "    with open(cause_file, 'r') as cause_infile:\n",
    "        reader = csv.DictReader(cause_infile)\n",
    "        for row in reader:\n",
    "            # Use the same key structure as in create_cause_dimension\n",
    "            unique_key = f\"{row['primary_cause']}|{row['secondary_cause']}|{row['road_condition']}|{row['lighting_condition']}|{row['weather_condition']}|{row['speed_limit']}|{row['traffic_control_device']}|{row['device_condition']}|{row['alignment']}|{row['road_defect']}\"\n",
    "            cause_mapping[unique_key] = row['cause_id']\n",
    "\n",
    "    # Process crash data and find cause_id\n",
    "    crashes = {}\n",
    "    with open(crashes_file, 'r') as infile:\n",
    "        reader = csv.DictReader(infile)\n",
    "        for row in reader:\n",
    "            crash_date = row['CRASH_DATE']\n",
    "            # Correct date parsing format\n",
    "            parsed_date = datetime.strptime(crash_date, '%m/%d/%Y %I:%M:%S %p')\n",
    "            date_id = parsed_date.strftime('%Y%m%d')\n",
    "\n",
    "            # Lookup cause_id from the Cause Dimension\n",
    "            unique_key = f\"{row['PRIM_CONTRIBUTORY_CAUSE']}|{row['SEC_CONTRIBUTORY_CAUSE']}|{row['ROADWAY_SURFACE_COND']}|{row['LIGHTING_CONDITION']}|{row['WEATHER_CONDITION']}|{row['POSTED_SPEED_LIMIT']}|{row['TRAFFIC_CONTROL_DEVICE']}|{row['DEVICE_CONDITION']}|{row['ALIGNMENT']}|{row['ROAD_DEFECT']}\"\n",
    "            cause_id = cause_mapping.get(unique_key, 'Unknown')  # Default to 'Unknown' if no match\n",
    "\n",
    "            crashes[row['RD_NO']] = {\n",
    "                'beat_id': row['BEAT_OF_OCCURRENCE'],\n",
    "                'date_id': date_id,\n",
    "                'cause_id': cause_id\n",
    "            }\n",
    "\n",
    "    # Process people data and link to crashes\n",
    "    with open(people_file, 'r') as infile:\n",
    "        reader = csv.DictReader(infile)\n",
    "        for row in reader:\n",
    "            crash_id = row['RD_NO']\n",
    "            person_id = row['PERSON_ID']\n",
    "            vehicle_id = row.get('VEHICLE_ID', 'Unknown')  # Default if missing\n",
    "            damage_category = row.get('DAMAGE_CATEGORY', 'Unknown')  # Default if missing\n",
    "            damage = row['DAMAGE']  # Assuming this is the correct field name for damage\n",
    "\n",
    "            # Ensure the crash exists in crashes data\n",
    "            if crash_id in crashes:\n",
    "                fact = {\n",
    "                    'crash_id': crash_id,\n",
    "                    'cause_id': crashes[crash_id]['cause_id'],  # Use cause_id from lookup\n",
    "                    'person_id': person_id,\n",
    "                    'vehicle_id': vehicle_id,\n",
    "                    'beat_id': crashes[crash_id]['beat_id'],\n",
    "                    'date_id': crashes[crash_id]['date_id'],\n",
    "                    'damage': damage,\n",
    "                    'damage_category': f'{float(damage_category):.2f}' if damage_category.replace('.', '', 1).isdigit() else damage_category\n",
    "                }\n",
    "                fact_table.append(fact)\n",
    "\n",
    "    # Write the Fact Table to the output file\n",
    "    with open(output_file, 'w', newline='') as outfile:\n",
    "        fieldnames = ['crash_id', 'cause_id', 'person_id', 'vehicle_id', 'beat_id', 'date_id', 'damage', 'damage_category']\n",
    "        writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(fact_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "create_date_dimension('Crashes_deduped.csv', 'DateDimension.csv')\n",
    "create_geography_dimension('Crashes_deduped.csv', 'GeographyDimension.csv')\n",
    "create_person_dimension('People_damage_filled_CSV.csv', 'PersonDimension.csv')\n",
    "create_cause_dimension('Crashes_deduped.csv', 'CauseDimension.csv')\n",
    "create_vehicle_dimension('Vehicles_IDfilled_nosplit.csv', 'VehicleDimension.csv' )\n",
    "create_crash_dimension('Crashes_deduped.csv', 'CrashDimension.csv')\n",
    "create_fact_table('Crashes_deduped.csv', 'People_damage_filled_CSV.csv', 'CauseDimension.csv', 'FactTable.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
